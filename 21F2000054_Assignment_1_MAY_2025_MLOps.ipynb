{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Deploying Iris-detection model using Vertex AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO",
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this tutorial, you build a scikit-learn model and deploy it on Vertex AI using the custom container method. You use the FastAPI Python web server framework to create a prediction endpoint. You also incorporate a preprocessor from training pipeline into your online serving application.\n",
    "\n",
    "Learn more about [Custom training](https://cloud.google.com/vertex-ai/docs/training/custom-training) and [Vertex AI Prediction](https://cloud.google.com/vertex-ai/docs/predictions/get-predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbd99f7bfc8e",
    "tags": []
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this notebook, you learn how to create, deploy and serve a custom classification model on Vertex AI. This notebook focuses more on deploying the model than on the design of the model itself. \n",
    "\n",
    "\n",
    "This tutorial uses the following Vertex AI services and resources:\n",
    "\n",
    "- Vertex AI models\n",
    "- Vertex AI endpoints\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Train a model that uses flower's measurements as input to predict the class of iris.\n",
    "- Save the model and its serialized pre-processor.\n",
    "- Build a FastAPI server to handle predictions and health checks.\n",
    "- Build a custom container with model artifacts.\n",
    "- Upload and deploy custom container to Vertex AI Endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fe0bb78c9ce",
    "tags": []
   },
   "source": [
    "### Dataset\n",
    "\n",
    "This tutorial uses R.A. Fisher's Iris dataset, a small and popular dataset for machine learning experiments. Each instance has four numerical features, which are different measurements of a flower, and a target label that\n",
    "categorizes the flower into: **Iris setosa**, **Iris versicolour** and **Iris virginica**.\n",
    "\n",
    "This tutorial uses [a version of the Iris dataset available in the\n",
    "scikit-learn library](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c681f532cf64"
   },
   "source": [
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "* Artifact Registry\n",
    "* Cloud Build\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), [Artifact Registry pricing](https://cloud.google.com/artifact-registry/pricing) and [Cloud Build pricing](https://cloud.google.com/build/pricing) and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0316df526f8"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9065e8d7f0fb"
   },
   "source": [
    "### Install Vertex AI SDK for Python and other required packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1fd00fa70a2a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Vertex SDK for Python\n",
    "! pip3 install --upgrade --quiet  google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfEglUHQk9S3"
   },
   "source": [
    "### Set Google Cloud project information \n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "set_project_id",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"wise-ally-461615-t2\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bucket",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://week1_assignment_1_lab_demo_iitm_bs\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "**If your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "create_bucket",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://week1_assignment_1_lab_demo_iitm_bs/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'week1_assignment_1_lab_demo_iitm_bs' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3330b4f12a0d"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "e088ea8cd4a0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3938f6d37a1"
   },
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "e95ca1e5e07c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Configure resource names\n",
    "\n",
    "Set a name for the following parameters:\n",
    "\n",
    "`MODEL_ARTIFACT_DIR` - Folder directory path to your model artifacts within a Cloud Storage bucket, for example: \"my-models/fraud-detection/trial-4\"\n",
    "\n",
    "`REPOSITORY` - Name of the Artifact Repository to create or use.\n",
    "\n",
    "`IMAGE` - Name of the container image that is pushed to the repository.\n",
    "\n",
    "`MODEL_DISPLAY_NAME` - Display name of Vertex AI model resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MzGDU7TWdts_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ARTIFACT_DIR = \"my-models/iris-classifier-week-1\"  # @param {type:\"string\"}\n",
    "REPOSITORY = \"iris-classifier-repo\"  # @param {type:\"string\"}\n",
    "IMAGE = \"iris-classifier-img\"  # @param {type:\"string\"}\n",
    "MODEL_DISPLAY_NAME = \"iris-classifier\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the defaults if no names were specified\n",
    "if MODEL_ARTIFACT_DIR == \"[your-artifact-directory]\":\n",
    "    MODEL_ARTIFACT_DIR = \"custom-container-prediction-model\"\n",
    "\n",
    "if REPOSITORY == \"[your-repository-name]\":\n",
    "    REPOSITORY = \"custom-container-prediction\"\n",
    "\n",
    "if IMAGE == \"[your-image-name]\":\n",
    "    IMAGE = \"sklearn-fastapi-server\"\n",
    "\n",
    "if MODEL_DISPLAY_NAME == \"[your-model-display-name]\":\n",
    "    MODEL_DISPLAY_NAME = \"sklearn-custom-container\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c2d091d9e73"
   },
   "source": [
    "## Simple Decision Tree model\n",
    "Build a Decision Tree model on iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "data = pd.read_csv('dataset/iris.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size = 0.4, stratify = data['species'], random_state = 42)\n",
    "X_train = train[['sepal_length','sepal_width','petal_length','petal_width']]\n",
    "y_train = train.species\n",
    "X_test = test[['sepal_length','sepal_width','petal_length','petal_width']]\n",
    "y_test = test.species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Decision Tree is 0.983\n"
     ]
    }
   ],
   "source": [
    "mod_dt = DecisionTreeClassifier(max_depth = 3, random_state = 1)\n",
    "mod_dt.fit(X_train,y_train)\n",
    "prediction=mod_dt.predict(X_test)\n",
    "print('The accuracy of the Decision Tree is',\"{:.3f}\".format(metrics.accuracy_score(prediction,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artifacts/model.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "\n",
    "joblib.dump(mod_dt, \"artifacts/model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3849066a33bd"
   },
   "source": [
    "### Upload model artifacts and custom code to Cloud Storage\n",
    "\n",
    "Before you can deploy your model for serving, Vertex AI needs access to the following files in Cloud Storage:\n",
    "\n",
    "* `model.joblib` (model artifact)\n",
    "* `preprocessor.pkl` (model artifact)\n",
    "\n",
    "Run the following commands to upload your files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ca67ee52d4d9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://artifacts/model.joblib [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  2.5 KiB/  2.5 KiB]                                                \n",
      "Operation completed over 1 objects/2.5 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp artifacts/model.joblib {BUCKET_URI}/{MODEL_ARTIFACT_DIR}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git config --global user.name \"SakethPatel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[32m⠋\u001b[0m Checking graph                                       core\u001b[39m>\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in dataset/iris.csv |0.00 [00:00,     ?fil\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/home/jupyter/.dvc/cache/files/md5'| |0/? [00:00<?,    ?\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /home/jupyter/dataset/iri0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|████████████████████████████████████████|1/1 [00:00, 30.21file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add dataset/iris.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc add dataset/iris.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: pathspec '.gitignore' did not match any files\n",
      "On branch master\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31m.bashrc\u001b[m\n",
      "\t\u001b[31m.cache/\u001b[m\n",
      "\t\u001b[31m.config/\u001b[m\n",
      "\t\u001b[31m.docker/\u001b[m\n",
      "\t\u001b[31m.gitconfig\u001b[m\n",
      "\t\u001b[31m.gsutil/\u001b[m\n",
      "\t\u001b[31m.ipynb_checkpoints/\u001b[m\n",
      "\t\u001b[31m.ipython/\u001b[m\n",
      "\t\u001b[31m.jupyter/\u001b[m\n",
      "\t\u001b[31m.local/\u001b[m\n",
      "\t\u001b[31m.npm/\u001b[m\n",
      "\t\u001b[31m21F2000054_Assignment_1_MAY_2025_MLOps.ipynb\u001b[m\n",
      "\t\u001b[31martifacts/\u001b[m\n",
      "\t\u001b[31mdataset/\u001b[m\n",
      "\t\u001b[31mnotebook_template.ipynb\u001b[m\n",
      "\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\n"
     ]
    }
   ],
   "source": [
    "!git add dataset/iris.csv.dvc .gitignore\n",
    "!git commit -m \"Track iris dataset with DVC\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://week1_assignment_2_lab_demo_iitm_bs/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'week1_assignment_2_lab_demo_iitm_bs' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "BUCKET = \"gs://week1_assignment_2_lab_demo_iitm_bs\"\n",
    "!gsutil mb -l us-central1 {BUCKET}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting 'myremote' as a default remote.\n",
      "\u001b[31mERROR\u001b[39m: configuration error - config file error: remote 'myremote' already exists. Use `-f|--force` to overwrite it.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc remote add -d myremote {BUCKET}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting                                            |1.00 [00:00,  157entry/s]\n",
      "Pushing\n",
      "!\u001b[A\n",
      "  0% Checking cache in 'week1_assignment_2_lab_demo_iitm_bs/files/md5'| |0/? [00\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/home/jupyter/.dvc/cache/files/md5'| |0/? [00:00<?,    ?\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Pushing to gs                         0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "  0%|          |Pushing to gs                         0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "\n",
      "!\u001b[A\u001b[A\n",
      "\n",
      "  0%|          |/home/jupyter/.dvc/cache/files/0.00/3.77k [00:00<?,        ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "Pushing                                                                         \u001b[A\n",
      "1 file pushed\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31m.bashrc\u001b[m\n",
      "\t\u001b[31m.cache/\u001b[m\n",
      "\t\u001b[31m.config/\u001b[m\n",
      "\t\u001b[31m.docker/\u001b[m\n",
      "\t\u001b[31m.gitconfig\u001b[m\n",
      "\t\u001b[31m.gsutil/\u001b[m\n",
      "\t\u001b[31m.ipynb_checkpoints/\u001b[m\n",
      "\t\u001b[31m.ipython/\u001b[m\n",
      "\t\u001b[31m.jupyter/\u001b[m\n",
      "\t\u001b[31m.local/\u001b[m\n",
      "\t\u001b[31m.npm/\u001b[m\n",
      "\t\u001b[31m21F2000054_Assignment_1_MAY_2025_MLOps.ipynb\u001b[m\n",
      "\t\u001b[31martifacts/\u001b[m\n",
      "\t\u001b[31mdataset/\u001b[m\n",
      "\t\u001b[31mnotebook_template.ipynb\u001b[m\n",
      "\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\n"
     ]
    }
   ],
   "source": [
    "!git add .dvc/config\n",
    "!git commit -m \"Configure DVC remote as GCS\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage 'train_model' in 'dvc.yaml'                               core\u001b[39m>\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[0mfatal: pathspec 'dvc.lock' did not match any files\n",
      "On branch master\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31m.bashrc\u001b[m\n",
      "\t\u001b[31m.cache/\u001b[m\n",
      "\t\u001b[31m.config/\u001b[m\n",
      "\t\u001b[31m.docker/\u001b[m\n",
      "\t\u001b[31m.gitconfig\u001b[m\n",
      "\t\u001b[31m.gsutil/\u001b[m\n",
      "\t\u001b[31m.ipynb_checkpoints/\u001b[m\n",
      "\t\u001b[31m.ipython/\u001b[m\n",
      "\t\u001b[31m.jupyter/\u001b[m\n",
      "\t\u001b[31m.local/\u001b[m\n",
      "\t\u001b[31m.npm/\u001b[m\n",
      "\t\u001b[31m21F2000054_Assignment_1_MAY_2025_MLOps.ipynb\u001b[m\n",
      "\t\u001b[31martifacts/\u001b[m\n",
      "\t\u001b[31mdataset/\u001b[m\n",
      "\t\u001b[31mdvc.yaml\u001b[m\n",
      "\t\u001b[31mnotebook_template.ipynb\u001b[m\n",
      "\t\u001b[31mtrain.py\u001b[m\n",
      "\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\n"
     ]
    }
   ],
   "source": [
    "!dvc stage add -n train_model \\\n",
    "  -d dataset/iris.csv -d train.py \\\n",
    "  -o artifacts/model.joblib \\\n",
    "  python train.py\n",
    "\n",
    "!git add dvc.yaml dvc.lock\n",
    "!git commit -m \"Add training pipeline to DVC\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'dataset/iris.csv.dvc' didn't change, skipping                                  \n",
      "Running stage 'train_model':                                                    \n",
      "> python train.py\n",
      "Generating lock file 'dvc.lock'                                                 \n",
      "Updating lock file 'dvc.lock'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add dvc.lock\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "Use `dvc push` to send your updates to remote storage.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc repro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master c25d9a3] Add lock file after running DVC pipeline\n",
      " 1 file changed, 18 insertions(+)\n",
      " create mode 100644 dvc.lock\n"
     ]
    }
   ],
   "source": [
    "!git add dvc.lock\n",
    "!git commit -m \"Add lock file after running DVC pipeline\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting                                            |2.00 [00:00,  213entry/s]\n",
      "Pushing\n",
      "!\u001b[A\n",
      "  0% Checking cache in 'week1_assignment_2_lab_demo_iitm_bs/files/md5'| |0/? [00\u001b[A\n",
      " 50% Querying cache in 'week1_assignment_2_lab_demo_iitm_bs/files/md5'|▌|1/2 [00\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/home/jupyter/.dvc/cache/files/md5'| |0/? [00:00<?,    ?\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Pushing to gs                         0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "\n",
      "!\u001b[A\u001b[A\n",
      "\n",
      "  0%|          |/home/jupyter/.dvc/cache/files/0.00/2.34k [00:00<?,        ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "100%|██████████|Pushing to gs                     1/1 [00:00<00:00,  8.79file/s]\u001b[A\n",
      "Pushing                                                                         \u001b[A\n",
      "1 file pushed\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SDK_Custom_Container_Prediction.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
